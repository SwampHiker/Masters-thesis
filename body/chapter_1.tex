\newpage
\begin{center}
	\textbf{\large ГЛАВА 1}

	\textbf{\large МАТРИЦЫ КОСИНУСОВ}
\end{center}
\refstepcounter{chapter}
\addcontentsline{toc}{chapter}{1. МАТРИЦЫ КОСИНУСОВ}

Матрицы косинусов как математический объект были предложены ранее в рамках курсовых и дипломной работ. Теоретические результаты предыдущих работ здесь будут изложены, по возможности, кратко и без доказательств, но систематично. Здесь они необходимы, прежде всего, для правильного изложения новых результатов, касающихся нового объекта -- релятивистской матрицы косинусов. Кроме того, терминология данной главы может местами отличаться от терминологии предыдущих работ, что связано с тем, что она ещё не устоялась, и требует некоторых изменений для большей систематичности.

\section{Базовые определения}
Пусть имеется некоторое гильбертово пространство $\mathcal{H}$ (в частном случае некоторое конечномерное евклидово пространство $\mathbb{R}^{k}$).
Пусть в данном пространстве задана последовательность точек (ломаная) $X = (\vec{x}_i)$, где $i = \overline{0,n}$. Обозначим $\vec{p}_{i} = \vec{x}_i - \vec{x}_{i-1}$, где $i = \overline{1,n}$, тогда \textit{ненормализованной матрицей косинусов} по ломаной $X$ будем называть квадратную матрицу вида:
\begin{equation}
	{C}_{E} = {(<\vec{p}_{i}, \vec{p}_{j}>)}_{ij} \in \mathbb{R}_{n,n}.
	\label{C_E_definition}
\end{equation}
Если выполняется условие:
\begin{equation}
	||\vec{p}_{i}|| = 1, i = \overline{1,n},
	\label{normalized_definition}
\end{equation}
то говорят о \textit{нормализованных матрицах косинусов} или собственно \textit{матрицах косинусов}. На практике, удобнее всего работать с нормализованными матрицами косинусов, ненормализованные матрицы косинусов могут возникнуть при некоторых операциях над обычными матрицами косинусов. Далее в работе, если не оговорено противное, будет подразумеваться наличие нормализации. 

Название матриц следует из того факта, что при условии \ref{normalized_definition}:
\begin{equation}
	{C}_{E} = {(<\vec{p}_{i}, \vec{p}_{j}>)}_{ij} = {(||\vec{p}_{i}|| ||\vec{p}_{j}|| \cos{\vec{p}_{i} \wedge \vec{p}_{j}})}_{ij} = {(\cos{\vec{p}_{i} \wedge \vec{p}_{j}})}_{ij}.
	\label{name_cosine}
\end{equation}
Объект, определенный выше, строится по и описывает одну ломаную. Пусть имеется две последовательности точек: $X' = (\vec{x'}_{i})$, где $i = \overline{0,m}$ и $X'' = (\vec{x''}_{j})$, где $j = \overline{0,n}$. Тогда аналогично \ref{C_E_definition} можно построить матрицу косинусов по двум ломаным $X'$ и $X''$:
\begin{equation}
	{C}_{X',X''} = {(<\vec{p'}_{i}, \vec{p''}_{j}>)}_{ij} \in \mathbb{R}_{m,n}.
	\label{C_X1X2_definition}
\end{equation}

Определения \ref{C_E_definition} и \ref{C_X1X2_definition} здесь фундаментальны, на практике полезны также матрица косинусов, кодирующая поворот ломаной $C_A$, и кодирующая вектор относительно ломаной ${C}_{\nu}$ -- они являются частными случаями \ref{C_X1X2_definition}. Здесь определим ${C}_{\nu}$, в следующей секции -- $C_A$:

Пусть задан некоторый вектор $\vec{\nu} \in \mathbb{R}^k$ и $X = (\vec{x}_{i})$ с $i = \overline{0,m}$, тогда \textit{матрицей косинусов задающей вектор} назовём матрицу вида:
\begin{equation}
	{C}_{\nu} = {(<\vec{p}_{i}, \vec{\nu}>)}_{ij} \in \mathbb{R}_{m,n},
	\label{C_nu_definition}
\end{equation}
здесь $n \ge 1$ не зависит от ни от $X$, ни от $\vec{\nu}$, а выбирается из других практических соображений.

Непрерывным обобщение матрицы косинусов можно назвать \textit{поверхностью косинусов}. Пусть $x(\alpha)$ - гладкая кривая $x : [0, l] \rightarrow \mathcal{H}$ и $\vec{p}(\alpha) = \frac{\partial{x(\alpha)}}{\partial{\alpha}}$. Тогда \ref{C_E_definition} можно переписать как:
\begin{equation}
	C({\alpha}_1, {\alpha}_2) = <\vec{p}(\alpha_1), \vec{p}(\alpha_2)>.
	\label{C_E_definition_cont}
\end{equation}
Таким образом, получаем действительную функцию о двух параметров $C : [0, l] \times [0, l] \rightarrow \mathbb{R}$.
Условие нормализации для \ref{C_E_definition_cont} выглядит следующим образом:
\begin{equation}
	||\vec{p}(\alpha)|| = 1, \forall{i} \in [0, l].
	\label{normalized_definition_cont}
\end{equation}
Использование непрерывной \ref{C_E_definition_cont} для вычислений затруденено. Гораздо удобнее исследовать и использовать матрицы.

\section{Свойства матриц косинусов}
Для исследования свойств описанных матриц, необходимо записать их в более удобном виде. Пусть речь идёт об евклидовом простанстве $\mathbb{R}^{k}$, тогда в нём можно ввести репер
$O = (\vec{x}_0, \vec{v}_1, ..., \vec{v}_k)$, пораждающий декартову систему координат. Тогда каждой точке $\vec{x}_i$ и вектору $\vec{p}_i$ можно поставить в соответствие координатный вектор-столбец $p_i \in \mathbb{R}_{k,1}$. Полученные столбцы координат $p_i$ можно объединить в матрицу $P = [p_1, ..., p_n] \in \mathbb{R}_{k,n}$. Тогда \ref{C_E_definition} и \ref{C_X1X2_definition} можно переписать как:
\begin{equation}
	C_E = P^\mathrm{T}P,
	\label{C_E_calc}
\end{equation}
\begin{equation}
	C_{X_1, X_2} = {P_1}^\mathrm{T}{P_2}.
	\label{C_X1X2_calc}
\end{equation}

Теперь, имея некоторую матрицу $A \in \mathbb{R}_{k, k}$ можно определить \textit{матрицу косинусов преобразования $A$}:
\begin{equation}
	C_A = P^\mathrm{T}AP.
	\label{C_A_calc}
\end{equation}

\subsection{Базовые свойства}
Приведём основные свойства матриц косинусов (без доказательств):
\begin{enumerate}
\item Все значения матриц косинусов лежат на отрезке $[-1, 1]$;
\item Все диагональные элементы матрицы $C_E$ равны $1$ (при отстутствии нормализации -- $||\vec{p}_i||$);
\item Матрицы косинусов не зависят от выбора декартовой системы координат;
\item Ранг матрицы $C_E$ равен размерности ломаной $X$;
\item Матрица $C_E$ - симметричная. Все её собственные значения неотрицательны, и в сумме равны $n$;
\item Ломаную $X$ (ломаные $X_1$ и $X_2$) можно восстановить из $C_E$ ($C_{X_1, X_2}$) с точностью до выбора системы координат, причём как для нормированных, так и для ненормированных матриц (в случае последних с $C_{X_1, X_2}$ необходимо знание длин векторов $\vec{p}_i$).
\end{enumerate}

\subsection{Восстановление исходных ломаных}
Последнее свойство наиболее важно, так как оно позволяет использовать матрицы косинусов как инвариантное представление геометричеких ломаных (что крайне полезно при исследовании белковых молекул и их взаимодействий). Зная \ref{C_E_calc} и \ref{C_X1X2_calc} легко построить соответсвующие матрицы. Ниже приведём без доказательств алгоритмы, которые позволяют строить ломаные по этим матрицам:

\begin{enumerate}
\item \textit{Восстановление $X$ из $C_E$}
Из описанного выше свойства 5 следует, что матрица $C_E$ будет обладать $k$ неотрицательными собственными значениями $\lambda_1, ..., \lambda_k$ и собственными векторами $v_1, ..., v_k \in \mathbb{R}_{n,1}$. Тогда в некоторой системе координат матрица $P$ примет вид:
\begin{equation}
	P = \diag(\sqrt{\lambda_1}, ..., \sqrt{\lambda_k}){[v_1, ..., v_k]}^\mathrm{T},
	\label{X_from_C_E_extraction}
\end{equation}
где координаты точек ломаной $X$ восстанавливаются из $P$ через кумулятивную сумму. Формула \ref{X_from_C_E_extraction} работает и для ненормализованных матриц.

\item \textit{Восстановление $X_2$ из $C_{X_1, X_2}$ при известном $X_1$}
Введём следующую крайне важную матрицу:
\begin{equation}
	T_P = {(P{P}^\mathrm{T})}^{-1}P,
	\label{T_definition}
\end{equation}
данная матрица (с поправкой на транспонирование) представляет собой матрицу вычисления параметров линейной регрессии. Из \ref{C_X1X2_calc} и \ref{T_definition} следует, что:
\begin{equation}
	T_{P_1}C_{X_1, X_2} = {(P_1{P_1}^\mathrm{T})}^{-1}P_1{P_1}^\mathrm{T}{P_2} = P_2,
	\label{X2_from_C_X1X2_extraction_known_X1}
\end{equation}
данная формула позволяет посчитать $P_2$ из $C_{X_1, X_2}$ при известном $P_1$, из которого, с помощью кумулятивной суммы, можно получить $X_2$. Формула \ref{X2_from_C_X1X2_extraction_known_X1}, опять же, работает и для ненормализованных матриц. Также с помощью такого подхожа можно восстановить вектор $\vec{\nu}$ из
\ref{C_nu_definition}.

\item \textit{Восстановление $A$ из $C_A$ при известном $X$}
Данная операция выполняется с помощью двойного применения матрицы \ref{T_definition}:
\begin{equation}
	T_PC_AT_P^\mathrm{T} = {(PP^\mathrm{T})}^{-1}P(P^\mathrm{T}AP)P^\mathrm{T}{(PP^\mathrm{T})}^{-1} = A,
	\label{A_from_C_A_extraction_known_X}
\end{equation}

\item \textit{Восстановление $X_1$ и $X_2$ из $C_{X_1, X_2}$}
Для выполнения данной операций существует следующий алгоритм, доказательство которого было приведено в предыдущей работе:
\begin{algorithmic}
\STATE 1.  Пусть $K_1$ -- матрица собственных векторов $C_{X_1,X_2}C^\mathrm{T}_{X_1,X_2}$;
\STATE 2.  Пусть ${K_2}$ -- матрица собственных векторов $C^\mathrm{T}_{X_1,X_2}C_{X_1,X_2}$;
\STATE 3.  $L_1 \gets K^\mathrm{T}_1C_{X_1,X_2}$;
\STATE 4.  $L_2 \gets (C_{X_1,X_2}K_2)^\mathrm{T}$;
\STATE 5.  $M \gets T_{L_2}C_{X_1,X_2}T^\mathrm{T}_{L_1}$;
\STATE 6.  Пусть $N$ -- матрица собственных значений $MM^\mathrm{T}$;
\STATE 7.  $O \gets N^\mathrm{T}ML_2$;
\STATE 8.  $s \gets T_{[O]^2}e$, где $e$ -- единичный вектор столбец;
\STATE 9.  $\hat{P}_1 = \diag(\Sqrt(s))O\diag(||\vec{p'}_1||, ..., ||\vec{p'}_m||)$;
\STATE 10. $\hat{P}_2 = T_{\hat{P}_1}C_{X_1,X_2}$.
\end{algorithmic}
\end{enumerate}

Описанные выше алгоритмы могут работать не только с истинными матрицами косинусов, но и с некоторыми приближёнными/зашумлёнными матрицами. Для этого достаточно производить нормализации по $||\vec{p}_i||$ там, где возникают матрицы $P$. В предыдущих работах было показана устойчивость описанных методов при добавлении случайных шумов.
 
\subsection{Арифметические свойства матриц косинусов}
\subsubsection{Сложение матриц $C_E$}
Пусть имеется две ломаные $X'$ в $\mathbb{R}^{k_1}$ и $X''$ в $\mathbb{R}^{k_2}$ имеющие соответствующие $P' = [p'_1, ..., p'_n] \in \mathbb{R}_{k_1,n}$ и $P'' = [p''_1, ..., p''_n] \in \mathbb{R}_{k_2, n}$.
Найдём сумму соответствующих им матриц $C'_E$ и $C''_E$:
\begin{equation}
	C'_E + C''_E = {P'}^\mathrm{T}P' + {P''}^\mathrm{T}P'' = {\begin{bmatrix} P' \\ P'' \end{bmatrix}}^\mathrm{T}{\begin{bmatrix} P' \\ P'' \end{bmatrix}} = {P^{+}}^\mathrm{T}P^{+} = C_E^{+},
	\label{C_E_sum}
\end{equation}
где получили новую ломанную $X^{+} = X' \oplus X''$ в $\mathbb{R}^{k_1 + k_2}$, а также $P^{+} = \begin{bmatrix} p'_1 & ... & p'_n \\ p''_1 & ... & p''_n \end{bmatrix} \in \mathbb{R}_{k_1 + k_2, n}$.
Таким образом сумма двух матриц косинусов -- ненормализованная матрица косинусов, которой соответствует ломаная, являющаяся прямой суммой ломаных-операнд.

\subsubsection{Вычитание матриц $C_E$}
Используя обозначения из \ref{C_E_sum} запишем:
\begin{equation}
	C'_E - C''_E = {P'}^\mathrm{T}P' - {P''}^\mathrm{T}P'' = {\begin{bmatrix} P' \\ iP'' \end{bmatrix}}^\mathrm{T}{\begin{bmatrix} P' \\ iP'' \end{bmatrix}} = {P^{-}}^\mathrm{T}P^{-} = C_E^{-}.
	\label{C_E_minus}
\end{equation}
Применяя разность в общем случае, придётся выйти из поля действительных чисел, и перейти к полю комплексных чисел. При том, что матрица $C_E^{-}$ всё еще действительная, порождающая её ломаная $X^{-} = X' \oplus iX''$ уже принадлежит $\mathbb{C}^{k_1 + k_2}$. Для матрицы, полученной из \ref{C_E_minus}, в общем случае не будет выполняться свойство о неотрицательности собственных значений. Из-за появления отрицательных собственных значений, при восстановлении $X^{-}$ из $C_E^{-}$ необходимо переписать \ref{X_from_C_E_extraction} как:
\begin{equation}
	P^{-} = \diag(\sqrt{\lambda_1^{+}}, ..., \sqrt{\lambda_{k_1}^{+}}, i\sqrt{-\lambda_1^{-}}, ..., i\sqrt{-\lambda_{k_2}^{-}}){[v_1^{+}, ..., v_{k_1}^{+}, v_1^{-}, ..., v_{k_2}^{-}]}^\mathrm{T}.
	\label{X_from_C_E_extraction_min}
\end{equation}

\subsubsection{Произведение матриц косинусов}
Раннее было показано, что произведение матриц косинусов может быть использовано для некоторых приближённых вычислений. Причины такого поведения, а также исследование
точности приближений здесь опустим, приведём лишь сами приближённые выражения.

Пусть $C_{X_1, X_2} \in \mathbb{R}_{m, n}$ и $C_{X_2, X_3} \in \mathbb{R}_{n, l}$, тогда:
\begin{equation}
	C_{X_1, X_2}C_{X_2, X_3} \approx \frac{n}{k}C_{X_1, X_3}.
	\label{mult_approx}
\end{equation}

Выражение \ref{mult_approx} приводит к следующим выражениям:
\begin{itemize}
\item $C_AC_B \approx \frac{n}{k}C_{AB}$;
\item Для ортогональных $A^\mathrm{T} = A^{-1}$ выполняется $C_A^\mathrm{T}C_A \approx C_AC_A^\mathrm{T} \approx \frac{n}{k}C_E$;
\item $C_EC_{X_1, X_2} \approx \frac{m}{k}C_{X_1, X_2}$;
\item $C_E^2 \approx \frac{n}{k}C_E$.
\end{itemize}

%\subsubsection{Применение свёрток к поверхностям косинусов}
%TO DO

%\subsection{Применение матриц косинусов}
%TO DO

\section{Релятивистская матрица косинусов}

В данном разделе представлен полностью новый результат, касающийся матриц косинусов. \textit{Релятивистская матрица косинусов $C_{rel}$} отличается от обычной $C_E$ тем, что позволяет закодировать не только лишь некоторую последовательность точек, но точку, из которой эта последовательность наблюдается. Таким образом получается инвариантное относительно выбора декартовой системы координат (а вообще говоря -- Лоренц-инвариантное) представление пары точка-ломаная, что позволяет задавать функции (поля), порождаемые ломаной (белковой молекулой), как преобразования матриц косинусов. Для релятивистских матриц будет дано полное определение, в соответствии со специальной теорией относительности (СТО), и приближённая запись, используемая на практике.

\subsection{Полное определение}

Пусть существует некоторая последовательность точек $X$, точки которой перемещаются в пространстве $\mathbb{R}^{k}$ со скоростями, не превышающими скорость света $c$:
\begin{align}
	X(t) = (\vec{x}_{l}(t)), l = \overline{0,n}; \\
	\vec{x}_l : \mathbb{R} \rightarrow \mathbb{R}^{k}; \notag \\
	||\frac{\partial{\vec{x}_l}}{\partial{t}}|| < c. \notag
\end{align}

Следуя Минковскому \cite{field_theory}, в пространстве-времени можно ввести систему координат, в которой точке $\vec{x}_l$ в момент времени $t$ будет поставлен в соответствие вектор-столбец $x_l = [x^1_l(t), ..., x^k_l(t), ict]^\mathrm{T} \in \mathbb{C}_{k+1,1}$.

Пусть имеется некоторая точка $\vec{x}_{ref}$ и момент времени $t_{ref}$, из которых наблюдается ломаная $X$. Этой точке в пространстве-времени соответствуют координаты $x_{ref} = [x^1_{ref}(t), ..., x^k_{ref}(t), ict_{ref}]^\mathrm{T}$. Каждая точка из $X$ будет наблюдаться в $\vec{x}_{ref}$ в момент $t_{ref}$ при пересечении траектории $\vec{x}_l(t)$ и светового конуса, порождённого $\vec{x}_{ref}$ и $t_{ref}$. Из того, что все скорости не превышают $c$, следует, что такое пересечение единственно, и для каждой точки $\vec{x}_l$ произойдёт в некоторый момент времени $t^*_l \le t_{ref}$. Таким образом, в $\vec{x}_{ref}$ в момент $t_{ref}$ будет наблюдаться $X^*$ - образ ломаной $X(t)$ вида:
$X^* = (\vec{x}^*_l)$, где $l = \overline{0,n}$ и координаты $\vec{x}^*_l$ равны $x^*_l = [x^1_l(t^*_l), ..., x^k_l(t^*_l), ict^*_l]^\mathrm{T}$.

Если теперь посчитать $\vec{p}_l = \vec{x}^*_l - \vec{x}^*_{l-1}$ и использовать произведение Минковского, \ref{C_E_definition} или \ref{C_E_calc} можно получить \textit{релятивистскую матрицу косинусов}:
\begin{equation}
	C_{rel} = {(<\vec{p}_{l}, \vec{p}_{j}>)}_{lj} = {\left(\sum_{s=1}^{k} p^s_l p^s_j - c^2(t^*_l - t^*_{l-1})(t^*_j - t^*_{j-1})\right)}_{lj} \in \mathbb{R}_{n,n}.
	\label{C_rel_definition}
\end{equation}

Отметим, что требование нормализации \ref{normalized_definition} для релятивистских матриц косинусов будет иметь вид:
 \begin{equation}
	\sum_{s=1}^{k} {(p^s_l)}^2 = 1, l = \overline{1,n},
	\label{normalized_definition_rel}
\end{equation}
т.е. единичную длину должны быть только первые $k$ действительных компонент $p_l$ -- мнимая компонента $p^{k+1}_l$ может быть любой.

\subsection{Упрощённая формулировка}
Теперь, для упрощения, положим, что $\forall{l}: \vec{x}_l(t) = \vec{x}_l = \const$. Также, не нарушая общности, положим $t_{ref} = 0$. При таких предположениях вычисление пересечения светового конуса и траекторий заметно упростится, и можно будет записать:
\begin{equation}
	t^*_l = t_{ref} - \frac{||\vec{x}_l - \vec{x}_{ref}||}{c} = -\frac{1}{c}\sqrt{\sum_{s=1}^{k}(x^s_l - x^s_{ref})^2},
	\label{t_formula}
\end{equation}
и на основании \ref{t_formula} получим следующие значения координат:
\begin{equation}
	x^*_l = \begin{bmatrix} x^1_l \\ ... \\ x^k_l \\ -i\sqrt{\sum_{s=1}^{k}(x^s_l - x^s_{ref})^2} \end{bmatrix}.
	\label{x_formula}
\end{equation}

По \ref{x_formula} можно посчитать $P^* = \begin{bmatrix} P \\ -id \end{bmatrix} \in \mathbb{C}_{k+1,n}$ и по \ref{C_E_calc} посчитать:
\begin{equation}
	C_{rel} = {\begin{bmatrix} P \\ -id \end{bmatrix}}^\mathrm{T}\begin{bmatrix} P \\ -id \end{bmatrix} = P^\mathrm{T}P - d^\mathrm{T}d = C_E - d^\mathrm{T}d \in \mathbb{R}_{n,n}.
	\label{C_rel_definition_practical}
\end{equation}

В случае, когда $||\frac{\partial{\vec{x}_l}}{\partial{t}}|| \ll c$ разница между $\vec{x}_l(t^*_l)$ и $\vec{x}_l(t_{ref})$ будет иметь порядок вычислительной погрешности, а следовательно  использование \ref{C_rel_definition} неоправдано. Далее свойства матриц косинусов будем исследовать исключительно на основе \ref{C_rel_definition_practical}.

Можно видеть, что по аналогии с обычной $C_E$ можно получить матрицы аналогичные $C_{X_1, X_2}$, $C_A$, $C_\nu$:
\begin{equation}
	C_{rel, X_1, X_2} = {P^*_1}^\mathrm{T}{P^*_2} = {\begin{bmatrix} P_1 \\ -id_1 \end{bmatrix}}^\mathrm{T}\begin{bmatrix} P_2 \\ -id_2 \end{bmatrix} = C_{X_1, X_2} - d_1^\mathrm{T}d_2 \in \mathbb{R}_{m,n},
	\label{C_rel_X1X2_calc}
\end{equation}
\begin{equation}
	C_{rel, A} = {P^*_1}^\mathrm{T}A{P^*_2} \in \mathbb{C}_{n,n}.
	\label{C_rel_A_calc}
\end{equation}
Если матрица $A \in \mathbb{C}_{k+1,k+1}$ имеет структуру:
\begin{equation}
	A = \begin{bmatrix} A_{11} & iA_{12} \\ iA_{21} & -A_{22} \end{bmatrix},
	\label{good_A}
\end{equation}
где $A_{11} \in \mathbb{R}_{k,k}$, $A_{12} \in \mathbb{R}_{k,1}$, $A_{21} \in \mathbb{R}_{1,k}$, $A_{22} \in \mathbb{R}_{1,1}$; то для \ref{C_rel_A_calc} можно гарантировать:
\begin{align}
	C_{rel, A} &= {\begin{bmatrix} P \\ -id \end{bmatrix}}^\mathrm{T}\begin{bmatrix} A_{11} & iA_{12} \\ iA_{21} & -A_{22} \end{bmatrix}\begin{bmatrix} P \\ -id \end{bmatrix} = \notag \\
			    &= C_{A_{11}} + d^\mathrm{T}A_{12}P + P^\mathrm{T}A_{21}d + d^\mathrm{T}A_{22}d \in \mathbb{R}_{n,n}.
	\label{C_rel_A_calc_real}
\end{align}

Аналогично, можно построить $C_{rel, \nu}$ и гарантировать, что она будет действительной, если $\nu = [\nu_1, ..., \nu_k, i\nu_{k+1}]^\mathrm{T}$ ($\nu_l \in \mathbb{R}$).

Матрицы $C_{rel, A}$ и $C_{rel, \nu}$ позволяют кодировать матрицы и векторы в пространстве-времени, также как и $C_A$, $C_\nu$ кодировали матрицы и векторы в евклидовом пространстве.
Кроме того, аналогично \ref{C_E_definition_cont} можно ввести релятивистскую поверхность косинусов. Этот объект пригодится для доказательства одного из свойств $C_{rel}$.

\subsection{Свойства релятивистских матриц косинусов}
Свойства релятивистских матриц аналогичны свойствам обычных матриц:
\begin{enumerate}
\item Все значения $C_{rel}$ лежат на отрезке $[-2, 2]$;
\item Все диагональные элементы матрицы $C_{rel}$ лежат на отрезке $[0, 1]$;
\item Релятивистские матрицы косинусов не меняются от преобразований Лоренца (см. \ref{Lorenz_group});
\item Матрица $C_{rel}$ - симметричная. Одно её собственное значение отрицательно, остальные -- неотрицательны.
\end{enumerate}
Приведём доказательство первых двух свойств, а потом отметим особенности восстановления структур из описанных матриц.

Первое доказательство дадим для непрерывного случая. Пусть $\vec{x}(\alpha)$ - гладкая кривая $\vec{x} : [0, l] \rightarrow \mathbb{R}^k$,
и пусть, не нарушая общности, она наблюдается из $x_{ref} = [0, 0, 0, 0]^\mathrm{T}$. Тогда координаты вектора $\vec{x}(\alpha)$ будут иметь вид:
\begin{equation}
	x(\alpha) = \begin{bmatrix} x_{real}(\alpha) \\ -i||x_{real}(\alpha)|| \end{bmatrix} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ -i\sqrt{x_1^2 + x_2^2 + x_3^2} \end{bmatrix},
\end{equation}
и тогда:
\begin{equation}
	p(\alpha) = \frac{\partial x(\alpha)}{\partial \alpha} = \begin{bmatrix} \frac{\partial x_1}{\partial \alpha} \\ \frac{\partial x_2}{\partial \alpha} \\ \frac{\partial x_3}{\partial \alpha} \\
	-i\frac{x_1\frac{\partial x_1}{\partial \alpha} + x_2\frac{\partial x_2}{\partial \alpha} + x_3\frac{\partial x_3}{\partial \alpha}}{\sqrt{x_1^2 + x_2^2 + x_3^2}} \end{bmatrix} =
	\begin{bmatrix} \frac{\partial x_{real}(\alpha)}{\partial \alpha} \\ -i\cos\left(x_{real}(\alpha) \wedge \frac{\partial x_{real}(\alpha)}{\partial \alpha}\right) \end{bmatrix}.
\end{equation}
Условие нормализации \ref{normalized_definition_rel} тут имеет вид:
\begin{equation}
	||\frac{\partial x_{real}}{\partial \alpha}|| = 1.
	\label{normalized_definition_rel_cont}
\end{equation}
Теперь, аналогично \ref{C_E_definition_cont}:
\begin{align}
	C_{rel}(\alpha_1, \alpha_2) &= <p(\alpha_1), p(\alpha_2)> =
	\cos\left(\frac{\partial x_{real}(\alpha_1)}{\partial \alpha}\wedge\frac{\partial x_{real}(\alpha_2)}{\partial \alpha}\right) - \notag \\
	&- \cos\left(x_{real}(\alpha_1) \wedge \frac{\partial x_{real}(\alpha_1)}{\partial \alpha}\right)\cos\left(x_{real}(\alpha_2) \wedge \frac{\partial x_{real}(\alpha_2)}{\partial \alpha}\right).
	\label{C_rel_value_cont}
\end{align}
Из \ref{C_rel_value_cont} очевидно следует, что:
\begin{equation}
	\forall{\alpha_1, \alpha_2}: -2 \le C_{rel}(\alpha_1, \alpha_2) \le 2,
\end{equation}
а для случая $\alpha_1 = \alpha_2$:
\begin{equation}
	C_{rel}(\alpha, \alpha) = 1 - \cos^2\left(x_{real}(\alpha) \wedge \frac{\partial x_{real}(\alpha)}{\partial \alpha}\right) = \sin^2\left(x_{real}(\alpha) \wedge \frac{\partial x_{real}(\alpha)}{\partial \alpha}\right),
\end{equation}
\begin{equation}
	\forall{\alpha}: 0 \le C_{rel}(\alpha, \alpha) \le 1.
	\label{C_rel_value_cont_end}
\end{equation}

Формула \ref{C_rel_value_cont} крайне любопытна, так как она даёт альтернативную аналитическую форму записи поверхности косинусов, имеющую довольно понятное трактование.
Предложенное доказательство верно для гладкого случая, однако, если речь идёт о ломаной (и, соотвественно, матрице, а не поверхности), следует дать другое доказательство, которое, впрочем, также довольно тривиальное.

Пусть имеется $X = (\vec{x}_l)$, где $l = \overline{0,n}$, которая будет наблюдаться из точки $O$. Для $\forall{l} \in \{1, ..., n\}$ можно обозначить $A = \vec{x}_{l-1}$ и $B = \vec{x}_{l}$.
Пусть $\overline{OA} = d$ и $\overline{OB} = d + \Delta$. Предполагая нормализацию, будем также иметь $\overline{AB} = 1$. По известному свойству треугольников, будем иметь:
\begin{align}
	\overline{OB} \le \overline{OA} + \overline{AB}, \notag \\
	d + \Delta \le d + 1, \notag \\
	\Delta \le 1,
	\label{delta_le_1}
\end{align}
и
\begin{align}
	\overline{OA} \le \overline{OB} + \overline{AB}, \notag \\
	d \le d + \Delta + 1, \notag \\
	-1 \le \Delta.
	\label{min1_le_delta}
\end{align}
Из \ref{delta_le_1} и \ref{min1_le_delta} следует $|\Delta| \le 1$. Тогда $C_{rel}$ запишется как:
\begin{equation}
	C_{rel} = {\left({\begin{bmatrix} p^{real}_l \\ -i\Delta_l \end{bmatrix}}^\mathrm{T}\begin{bmatrix} p^{real}_j \\ -i\Delta_j \end{bmatrix}\right)}_{lj} = {({p^{real}_l}^\mathrm{T}p^{real}_j - \Delta_l\Delta_j)}_{lj},
\end{equation}
откуда аналогично \ref{C_rel_value_cont}-\ref{C_rel_value_cont_end} получаем первые 2 свойства.

\subsubsection{Восстановление исходных ломаных из релятивистских матриц}
Формула \ref{A_from_C_A_extraction_known_X}, а также \ref{X2_from_C_X1X2_extraction_known_X1} (для $C_{rel,\nu}$) применяются здесь так же, как и для обычных матриц косинусов.
В то же время применение \ref{X_from_C_E_extraction} затруднено, и хотя формула \ref{X_from_C_E_extraction_min}, казалось бы, здесь применима,
но наличие элемента $-i\sqrt{\sum_{s=1}^{k}(x^s_l - x^s_{ref})^2}$ в \ref{x_formula} приводит к тому,
что требуются некоторые дополнительные преобразования для согласования координат точек $\vec{x}_l$ и их расстояний до точки наблюдения.
Задача несколько упрощается, если известна структура ломаной, и необходимо восстановить только позицию точки наблюдения.

Хотя описанную выше задачу можно сформулировать и решать в терминах задачи оптимизации, эффективного алгоритма её решения пока не предложено.
Эксперименты показали, что её можно эффективно решать с помощью нейросетевых алгоритмов.
% TO DO расписать подробно
